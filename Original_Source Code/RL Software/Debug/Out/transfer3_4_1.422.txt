
>>> Values:
 Random Seed Value = 48388
 Number of Physical Paths = 5
 Number of Learning Paths per Physical Path = 1
 Method = 5
 Number of Lookahead Levels = 1
 Path-File Mode = 3
 Step-Size Parameter ALPHA = 0.100000
 Discount Rate GAMMA = 0.900000
 Lookahead Weight Factor LAMBDA = 0.500000
 Q-value Weight Factor w1 = 0.800000
 Distance-to-Goal Weight Factor w2 = 0.500000
 Initial Full Exploration is OFF
 Visited-Number Weight factor w3 = 0.500000
 TAW has greedy-mode value = 1
 Star is 4 at location [1, 0]
 Goal is W

>>> Input Maze:
 O  O  O  O  O  O  O  B 
 O  O  O  O  B  O  O  O 
 O  O  O  B  O  O  O  O 
 O  B  O  O  O  B  O  O 
 O  B  O  B  O  B  O  O 
 O  O  O  O  O  O  O  B 


>>> Agent Physical Path 2:

>>> Physical path 2, Time: 35.408000 s, Min radius: 1.000000 Total Steps: 121.
 Step   1: Agent moved DOWN_RIGHT to location [x=2,y=1], based on policy.
 Step   2: Agent moved RIGHT to location [x=3,y=1], based on policy.
 Step   3: Agent moved UP_RIGHT to location [x=4,y=0], based on policy.
 Step   4: Agent moved DOWN_LEFT to location [x=3,y=1], randomly.
 Step   5: Agent moved  LEFT to location [x=2,y=1], based on policy.
 Step   6: Agent moved UP_LEFT to location [x=1,y=0], based on policy.
 Step   7: Agent moved  LEFT to location [x=0,y=0], based on policy.
 Step   8: Agent moved DOWN_RIGHT to location [x=1,y=1], randomly.
 Step   9: Agent moved RIGHT to location [x=2,y=1], based on policy.
 Step  10: Agent moved UP_RIGHT to location [x=3,y=0], based on policy.
 Step  11: Agent moved RIGHT to location [x=4,y=0], randomly.
 Step  12: Agent moved DOWN_RIGHT to location [x=5,y=1], based on policy.
 Step  13: Agent moved  DOWN to location [x=5,y=2], based on policy.
 Step  14: Agent moved DOWN_LEFT to location [x=4,y=3], based on policy.
 Step  15: Agent moved  LEFT to location [x=3,y=3], based on policy.
 Step  16: Agent moved UP_LEFT to location [x=2,y=2], based on policy.
 Step  17: Agent moved UP_LEFT to location [x=1,y=1], based on policy.
 Step  18: Agent moved UP_LEFT to location [x=0,y=0], based on policy.
 Step  19: Agent moved RIGHT to location [x=1,y=0], randomly.
 Step  20: Agent moved DOWN_RIGHT to location [x=2,y=1], based on policy.
 Step  21: Agent moved RIGHT to location [x=3,y=1], based on policy.
 Step  22: Agent moved DOWN_RIGHT to location [x=4,y=2], based on policy.
 Step  23: Agent moved RIGHT to location [x=5,y=2], based on policy.
 Step  24: Agent moved UP_RIGHT to location [x=6,y=1], based on policy.
 Step  25: Agent moved    UP to location [x=6,y=0], based on policy.
 Step  26: Agent moved  DOWN to location [x=6,y=1], randomly.
 Step  27: Agent moved DOWN_LEFT to location [x=5,y=2], based on policy.
 Step  28: Agent moved  LEFT to location [x=4,y=2], based on policy.
 Step  29: Agent moved UP_LEFT to location [x=3,y=1], based on policy.
 Step  30: Agent moved  LEFT to location [x=2,y=1], based on policy.
 Step  31: Agent moved DOWN_LEFT to location [x=1,y=2], based on policy.
 Step  32: Agent moved  LEFT to location [x=0,y=2], based on policy.
 Step  33: Agent moved  DOWN to location [x=0,y=3], randomly.
 Step  34: Agent moved UP_RIGHT to location [x=1,y=2], randomly.
 Step  35: Agent moved    UP to location [x=1,y=1], based on policy.
 Step  36: Agent moved UP_LEFT to location [x=0,y=0], based on policy.
 Step  37: Agent moved  DOWN to location [x=0,y=1], randomly.
 Step  38: Agent moved  DOWN to location [x=0,y=2], based on policy.
 Step  39: Agent moved RIGHT to location [x=1,y=2], randomly.
 Step  40: Agent moved UP_RIGHT to location [x=2,y=1], based on policy.
 Step  41: Agent moved    UP to location [x=2,y=0], based on policy.
 Step  42: Agent moved DOWN_RIGHT to location [x=3,y=1], randomly.
 Step  43: Agent moved DOWN_LEFT to location [x=2,y=2], randomly.
 Step  44: Agent moved  LEFT to location [x=1,y=2], based on policy.
 Step  45: Agent moved DOWN_LEFT to location [x=0,y=3], based on policy.
 Step  46: Agent moved UP_RIGHT to location [x=1,y=2], randomly.
 Step  47: Agent moved    UP to location [x=1,y=1], based on policy.
 Step  48: Agent moved UP_LEFT to location [x=0,y=